---
title: "Supervised Learning with scikit-learn"
format: html
jupyter: python3
---

## libraries

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

## most used commands from sklearn

#### model_selection module

```{python}
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
```

#### linear_model

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import LogisticRegression
```

#### metrics

```{python}
from sklearn.metrics import mean_squared_error
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
```

#### imputations

```{python}
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

```

#### preprocessing

```{python}
from sklearn.preprocessing import StandardScaler
```

#### decision trees

```{python}
from sklearn.tree import DecisionTreeClassifier
```

## steps in modeling

-   create X (features) and y (target)

-   split data: training features, test features, training target, testing target

-   scale the training data using training statistics

-   scale the testing data using the training statistics

-   create a hash map/dictionary with models names as dictionary keys, instantiate of models as dictionary values

-   loop through models (dictionary values)

-   within loop, do cross validation with scaled training data

-   store model performance metrics (accuracy, mean squared error etc)

-   boxplot the cv results for each model

-   loop through fitted models and make predictions on test data

-   store prediction metrics

-   compare results

## data

```{python}
remote = 'https://raw.githubusercontent.com/'
account = 'turalsadigov/'
folder = 'MATH_254/main/data/'
file = 'county.csv'
url = remote + account + folder + file
county = pd.read_csv(url)
county.head()
```

Type of the data loaded.

```{python}
type(county)
```

## missing data

```{python}
county.isna().head()
county.isna().sum()
county.isna().sum().sort_values(ascending=False)
```

#### option 1: Drop all rows with missing entries (wasteful!)

```{python}
county.shape
county.dropna().shape
```

#### option 2: Imputation (make educated guesses for the missing values)

Some simple approaches to imputation.

-   numerical variables: use mean or median

-   categorical variables: use mode

Note: train-test split before imputation to prevent '`data leakage`'.

```{python}
# get info on datay
county.info()
county = county.loc[:, county.columns != 'smoking_ban'].dropna()
# select categorical variables
X_cat = county.select_dtypes(include=['object']).drop('name', axis = 1).values
# select numerical variables (except target: median household income)
X_num = county.select_dtypes(include=np.number).drop('median_hh_income', axis = 1).values
# select target
y = county['median_hh_income'].values
# train-test split
X_train_cat, X_test_cat, y_train, y_test = train_test_split(X_cat, y, test_size = 0.2, random_state = 12)
                                                            
X_train_num, X_test_num, y_train, y_test = train_test_split(X_num, y, 
                                                            test_size = 0.2, 
                                                            random_state = 12)

# imputation of categorical variables
imp_cat = SimpleImputer(strategy = 'most_frequent')
X_train_cat = imp_cat.fit_transform(X_train_cat)
X_test_cat = imp_cat.transform(X_test_cat)

# imputation of numerical variables
                                                  
imp_num = SimpleImputer(strategy = 'mean') 
X_train_num = imp_num.fit_transform(X_train_num)
X_test_num = imp_num.transform(X_test_num)

# combine training data

X_train = np.append(X_train_num, X_train_cat, axis = 1)
X_test = np.append(X_test_num, X_test_cat, axis = 1)
```


```{python}
X_train = pd.get_dummies(pd.DataFrame(X_train))
X_test = pd.get_dummies(pd.DataFrame(X_test))
```


Lets fit a model

```{python}
#| eval: false
my_model = LinearRegression()
my_model.fit(X_train, y_train)
y_pred = my_model.predict(X_test)
mean_squared_error(y_test, y_pred)
```
